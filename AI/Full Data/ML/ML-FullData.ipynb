{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CH1</th>\n",
       "      <th>CH2</th>\n",
       "      <th>CH3</th>\n",
       "      <th>CH4</th>\n",
       "      <th>CH5</th>\n",
       "      <th>CH6</th>\n",
       "      <th>CH7</th>\n",
       "      <th>CH8</th>\n",
       "      <th>CH9</th>\n",
       "      <th>CH10</th>\n",
       "      <th>...</th>\n",
       "      <th>CH120</th>\n",
       "      <th>CH121</th>\n",
       "      <th>CH122</th>\n",
       "      <th>CH123</th>\n",
       "      <th>CH124</th>\n",
       "      <th>CH125</th>\n",
       "      <th>CH126</th>\n",
       "      <th>CH127</th>\n",
       "      <th>CH128</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.356582</td>\n",
       "      <td>-4.183985</td>\n",
       "      <td>-10.746856</td>\n",
       "      <td>-6.267521</td>\n",
       "      <td>-3.797519</td>\n",
       "      <td>-2.302936</td>\n",
       "      <td>-1.252548</td>\n",
       "      <td>-7.746185</td>\n",
       "      <td>0.471738</td>\n",
       "      <td>-6.161901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020039</td>\n",
       "      <td>-2.472429</td>\n",
       "      <td>8.327164</td>\n",
       "      <td>-7.159484</td>\n",
       "      <td>-7.407739</td>\n",
       "      <td>-3.116786</td>\n",
       "      <td>-5.184181</td>\n",
       "      <td>-3.349719</td>\n",
       "      <td>-3.648696</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.945604</td>\n",
       "      <td>-3.306842</td>\n",
       "      <td>-10.491301</td>\n",
       "      <td>-5.355060</td>\n",
       "      <td>-4.204535</td>\n",
       "      <td>-2.451794</td>\n",
       "      <td>-1.078876</td>\n",
       "      <td>-6.614024</td>\n",
       "      <td>0.476950</td>\n",
       "      <td>-6.373581</td>\n",
       "      <td>...</td>\n",
       "      <td>1.339626</td>\n",
       "      <td>-1.503563</td>\n",
       "      <td>8.399263</td>\n",
       "      <td>-6.161061</td>\n",
       "      <td>-6.157439</td>\n",
       "      <td>-1.761651</td>\n",
       "      <td>-3.521291</td>\n",
       "      <td>-1.393808</td>\n",
       "      <td>-2.770334</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.374339</td>\n",
       "      <td>-3.210059</td>\n",
       "      <td>-10.944341</td>\n",
       "      <td>-5.595750</td>\n",
       "      <td>-5.207158</td>\n",
       "      <td>-2.915861</td>\n",
       "      <td>-1.167011</td>\n",
       "      <td>-5.487274</td>\n",
       "      <td>-1.091632</td>\n",
       "      <td>-7.736202</td>\n",
       "      <td>...</td>\n",
       "      <td>2.109172</td>\n",
       "      <td>-0.861245</td>\n",
       "      <td>7.858149</td>\n",
       "      <td>-5.850475</td>\n",
       "      <td>-5.879990</td>\n",
       "      <td>-0.573228</td>\n",
       "      <td>-2.107724</td>\n",
       "      <td>-0.457397</td>\n",
       "      <td>-2.487843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.022699</td>\n",
       "      <td>-4.214879</td>\n",
       "      <td>-12.047193</td>\n",
       "      <td>-7.136577</td>\n",
       "      <td>-6.670796</td>\n",
       "      <td>-3.609811</td>\n",
       "      <td>-1.532931</td>\n",
       "      <td>-5.089931</td>\n",
       "      <td>-3.724341</td>\n",
       "      <td>-10.143687</td>\n",
       "      <td>...</td>\n",
       "      <td>1.334883</td>\n",
       "      <td>-1.450996</td>\n",
       "      <td>6.402653</td>\n",
       "      <td>-6.514009</td>\n",
       "      <td>-6.857675</td>\n",
       "      <td>-0.646044</td>\n",
       "      <td>-1.901586</td>\n",
       "      <td>-1.285065</td>\n",
       "      <td>-3.199575</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.317593</td>\n",
       "      <td>-6.341699</td>\n",
       "      <td>-13.596499</td>\n",
       "      <td>-9.721630</td>\n",
       "      <td>-8.280229</td>\n",
       "      <td>-4.351480</td>\n",
       "      <td>-2.061769</td>\n",
       "      <td>-6.095693</td>\n",
       "      <td>-6.715143</td>\n",
       "      <td>-13.108866</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.394790</td>\n",
       "      <td>-3.764086</td>\n",
       "      <td>3.939228</td>\n",
       "      <td>-8.125721</td>\n",
       "      <td>-8.944704</td>\n",
       "      <td>-2.570611</td>\n",
       "      <td>-3.342424</td>\n",
       "      <td>-3.797954</td>\n",
       "      <td>-4.744012</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-11.347833</td>\n",
       "      <td>-9.045066</td>\n",
       "      <td>-15.138646</td>\n",
       "      <td>-12.573982</td>\n",
       "      <td>-9.571043</td>\n",
       "      <td>-4.902422</td>\n",
       "      <td>-2.538497</td>\n",
       "      <td>-8.844329</td>\n",
       "      <td>-9.460058</td>\n",
       "      <td>-15.817164</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.473533</td>\n",
       "      <td>-7.390439</td>\n",
       "      <td>0.964761</td>\n",
       "      <td>-10.181576</td>\n",
       "      <td>-11.441631</td>\n",
       "      <td>-5.938907</td>\n",
       "      <td>-6.254167</td>\n",
       "      <td>-6.961302</td>\n",
       "      <td>-6.451284</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-13.727580</td>\n",
       "      <td>-11.357480</td>\n",
       "      <td>-16.027704</td>\n",
       "      <td>-14.608987</td>\n",
       "      <td>-10.085396</td>\n",
       "      <td>-5.042579</td>\n",
       "      <td>-2.739409</td>\n",
       "      <td>-13.017964</td>\n",
       "      <td>-11.537113</td>\n",
       "      <td>-17.298231</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.589652</td>\n",
       "      <td>-11.210074</td>\n",
       "      <td>-1.580985</td>\n",
       "      <td>-11.847825</td>\n",
       "      <td>-13.305220</td>\n",
       "      <td>-9.562409</td>\n",
       "      <td>-9.947935</td>\n",
       "      <td>-9.398963</td>\n",
       "      <td>-7.528073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-15.657780</td>\n",
       "      <td>-12.296570</td>\n",
       "      <td>-15.635710</td>\n",
       "      <td>-14.858233</td>\n",
       "      <td>-9.544531</td>\n",
       "      <td>-4.646577</td>\n",
       "      <td>-2.540309</td>\n",
       "      <td>-17.410857</td>\n",
       "      <td>-12.564220</td>\n",
       "      <td>-16.694256</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.427728</td>\n",
       "      <td>-13.959871</td>\n",
       "      <td>-2.802353</td>\n",
       "      <td>-12.287063</td>\n",
       "      <td>-13.552210</td>\n",
       "      <td>-12.089572</td>\n",
       "      <td>-13.301622</td>\n",
       "      <td>-10.268582</td>\n",
       "      <td>-7.472611</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-16.065189</td>\n",
       "      <td>-11.165067</td>\n",
       "      <td>-13.529844</td>\n",
       "      <td>-12.794677</td>\n",
       "      <td>-7.907675</td>\n",
       "      <td>-3.700201</td>\n",
       "      <td>-1.956622</td>\n",
       "      <td>-20.102499</td>\n",
       "      <td>-12.029400</td>\n",
       "      <td>-13.548552</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.070301</td>\n",
       "      <td>-14.628959</td>\n",
       "      <td>-2.124832</td>\n",
       "      <td>-10.925930</td>\n",
       "      <td>-11.589879</td>\n",
       "      <td>-12.423819</td>\n",
       "      <td>-14.970184</td>\n",
       "      <td>-9.581035</td>\n",
       "      <td>-6.211742</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-13.888465</td>\n",
       "      <td>-7.641836</td>\n",
       "      <td>-9.538939</td>\n",
       "      <td>-8.422123</td>\n",
       "      <td>-5.318664</td>\n",
       "      <td>-2.273712</td>\n",
       "      <td>-1.096104</td>\n",
       "      <td>-19.202040</td>\n",
       "      <td>-9.303129</td>\n",
       "      <td>-8.006057</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.987175</td>\n",
       "      <td>-12.565097</td>\n",
       "      <td>0.758154</td>\n",
       "      <td>-7.576129</td>\n",
       "      <td>-7.372830</td>\n",
       "      <td>-9.866407</td>\n",
       "      <td>-13.868813</td>\n",
       "      <td>-7.760791</td>\n",
       "      <td>-3.935865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-8.588481</td>\n",
       "      <td>-1.863644</td>\n",
       "      <td>-3.815619</td>\n",
       "      <td>-2.247339</td>\n",
       "      <td>-2.053280</td>\n",
       "      <td>-0.499184</td>\n",
       "      <td>-0.085704</td>\n",
       "      <td>-13.846472</td>\n",
       "      <td>-3.961860</td>\n",
       "      <td>-0.838915</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.072401</td>\n",
       "      <td>-7.641586</td>\n",
       "      <td>5.876706</td>\n",
       "      <td>-2.504855</td>\n",
       "      <td>-1.445197</td>\n",
       "      <td>-4.316758</td>\n",
       "      <td>-9.762311</td>\n",
       "      <td>-5.106575</td>\n",
       "      <td>-0.913842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.701868</td>\n",
       "      <td>5.383435</td>\n",
       "      <td>3.051153</td>\n",
       "      <td>4.742328</td>\n",
       "      <td>1.470267</td>\n",
       "      <td>1.421057</td>\n",
       "      <td>0.960833</td>\n",
       "      <td>-4.814419</td>\n",
       "      <td>3.748668</td>\n",
       "      <td>6.728548</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013180</td>\n",
       "      <td>-0.602847</td>\n",
       "      <td>12.664744</td>\n",
       "      <td>3.505577</td>\n",
       "      <td>5.106148</td>\n",
       "      <td>3.389623</td>\n",
       "      <td>-3.515525</td>\n",
       "      <td>-1.797644</td>\n",
       "      <td>2.555682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.026706</td>\n",
       "      <td>12.599540</td>\n",
       "      <td>9.969863</td>\n",
       "      <td>11.198011</td>\n",
       "      <td>4.698724</td>\n",
       "      <td>3.209242</td>\n",
       "      <td>1.933492</td>\n",
       "      <td>5.631706</td>\n",
       "      <td>12.654003</td>\n",
       "      <td>13.333624</td>\n",
       "      <td>...</td>\n",
       "      <td>8.645466</td>\n",
       "      <td>6.836127</td>\n",
       "      <td>19.674685</td>\n",
       "      <td>9.263953</td>\n",
       "      <td>10.891760</td>\n",
       "      <td>11.434667</td>\n",
       "      <td>3.302683</td>\n",
       "      <td>1.784651</td>\n",
       "      <td>6.105044</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.225856</td>\n",
       "      <td>17.975757</td>\n",
       "      <td>15.551407</td>\n",
       "      <td>15.781702</td>\n",
       "      <td>7.075807</td>\n",
       "      <td>4.568995</td>\n",
       "      <td>2.729387</td>\n",
       "      <td>14.566747</td>\n",
       "      <td>20.819525</td>\n",
       "      <td>17.839839</td>\n",
       "      <td>...</td>\n",
       "      <td>14.736543</td>\n",
       "      <td>12.552038</td>\n",
       "      <td>24.906468</td>\n",
       "      <td>13.552375</td>\n",
       "      <td>14.696814</td>\n",
       "      <td>17.640820</td>\n",
       "      <td>9.112618</td>\n",
       "      <td>4.984274</td>\n",
       "      <td>9.304216</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.898332</td>\n",
       "      <td>20.188929</td>\n",
       "      <td>18.620399</td>\n",
       "      <td>17.699510</td>\n",
       "      <td>8.263268</td>\n",
       "      <td>5.296659</td>\n",
       "      <td>3.285915</td>\n",
       "      <td>19.454721</td>\n",
       "      <td>26.151698</td>\n",
       "      <td>19.655002</td>\n",
       "      <td>...</td>\n",
       "      <td>17.738284</td>\n",
       "      <td>15.013199</td>\n",
       "      <td>26.742834</td>\n",
       "      <td>15.638798</td>\n",
       "      <td>15.989765</td>\n",
       "      <td>20.442837</td>\n",
       "      <td>12.751268</td>\n",
       "      <td>7.183486</td>\n",
       "      <td>11.717796</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18.268234</td>\n",
       "      <td>19.040525</td>\n",
       "      <td>18.739086</td>\n",
       "      <td>17.047463</td>\n",
       "      <td>8.284639</td>\n",
       "      <td>5.369881</td>\n",
       "      <td>3.608497</td>\n",
       "      <td>19.031152</td>\n",
       "      <td>27.226232</td>\n",
       "      <td>18.899846</td>\n",
       "      <td>...</td>\n",
       "      <td>17.346286</td>\n",
       "      <td>13.957641</td>\n",
       "      <td>24.784764</td>\n",
       "      <td>15.568981</td>\n",
       "      <td>15.134221</td>\n",
       "      <td>19.546347</td>\n",
       "      <td>13.620667</td>\n",
       "      <td>8.133699</td>\n",
       "      <td>12.967944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.042345</td>\n",
       "      <td>15.510717</td>\n",
       "      <td>16.433205</td>\n",
       "      <td>14.716613</td>\n",
       "      <td>7.481322</td>\n",
       "      <td>4.946508</td>\n",
       "      <td>3.744095</td>\n",
       "      <td>13.790523</td>\n",
       "      <td>23.997702</td>\n",
       "      <td>16.359146</td>\n",
       "      <td>...</td>\n",
       "      <td>14.361574</td>\n",
       "      <td>10.290956</td>\n",
       "      <td>19.982668</td>\n",
       "      <td>14.055664</td>\n",
       "      <td>13.159627</td>\n",
       "      <td>15.863175</td>\n",
       "      <td>11.837364</td>\n",
       "      <td>7.938288</td>\n",
       "      <td>12.866485</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.078262</td>\n",
       "      <td>11.279956</td>\n",
       "      <td>13.003575</td>\n",
       "      <td>11.953432</td>\n",
       "      <td>6.343219</td>\n",
       "      <td>4.290719</td>\n",
       "      <td>3.731271</td>\n",
       "      <td>5.920555</td>\n",
       "      <td>18.034664</td>\n",
       "      <td>13.212865</td>\n",
       "      <td>...</td>\n",
       "      <td>10.048368</td>\n",
       "      <td>5.486418</td>\n",
       "      <td>14.098517</td>\n",
       "      <td>12.096164</td>\n",
       "      <td>11.271009</td>\n",
       "      <td>10.961098</td>\n",
       "      <td>8.404723</td>\n",
       "      <td>6.800525</td>\n",
       "      <td>11.593885</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.714044</td>\n",
       "      <td>8.097053</td>\n",
       "      <td>10.056000</td>\n",
       "      <td>9.890903</td>\n",
       "      <td>5.352487</td>\n",
       "      <td>3.681812</td>\n",
       "      <td>3.585728</td>\n",
       "      <td>-1.465411</td>\n",
       "      <td>12.038799</td>\n",
       "      <td>10.631726</td>\n",
       "      <td>...</td>\n",
       "      <td>5.664882</td>\n",
       "      <td>1.050045</td>\n",
       "      <td>9.040040</td>\n",
       "      <td>10.583893</td>\n",
       "      <td>10.395355</td>\n",
       "      <td>6.505560</td>\n",
       "      <td>5.002209</td>\n",
       "      <td>4.989876</td>\n",
       "      <td>9.746565</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.070165</td>\n",
       "      <td>7.260290</td>\n",
       "      <td>8.960295</td>\n",
       "      <td>9.256028</td>\n",
       "      <td>4.892503</td>\n",
       "      <td>3.345529</td>\n",
       "      <td>3.329523</td>\n",
       "      <td>-5.621351</td>\n",
       "      <td>8.754019</td>\n",
       "      <td>9.387173</td>\n",
       "      <td>...</td>\n",
       "      <td>2.339594</td>\n",
       "      <td>-1.762937</td>\n",
       "      <td>6.373632</td>\n",
       "      <td>10.065965</td>\n",
       "      <td>10.926466</td>\n",
       "      <td>3.827274</td>\n",
       "      <td>3.160027</td>\n",
       "      <td>2.995303</td>\n",
       "      <td>8.098228</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.567974</td>\n",
       "      <td>9.223716</td>\n",
       "      <td>10.362699</td>\n",
       "      <td>10.209888</td>\n",
       "      <td>5.165280</td>\n",
       "      <td>3.394858</td>\n",
       "      <td>3.024490</td>\n",
       "      <td>-5.369264</td>\n",
       "      <td>9.729211</td>\n",
       "      <td>9.627102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909194</td>\n",
       "      <td>-2.231149</td>\n",
       "      <td>6.893173</td>\n",
       "      <td>10.617747</td>\n",
       "      <td>12.648485</td>\n",
       "      <td>3.531653</td>\n",
       "      <td>3.370974</td>\n",
       "      <td>1.532905</td>\n",
       "      <td>7.198523</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.933700</td>\n",
       "      <td>13.328832</td>\n",
       "      <td>13.868129</td>\n",
       "      <td>12.271233</td>\n",
       "      <td>6.092496</td>\n",
       "      <td>3.776579</td>\n",
       "      <td>2.749331</td>\n",
       "      <td>-1.542197</td>\n",
       "      <td>14.510664</td>\n",
       "      <td>10.884648</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537277</td>\n",
       "      <td>-0.495630</td>\n",
       "      <td>10.211183</td>\n",
       "      <td>11.816085</td>\n",
       "      <td>14.814487</td>\n",
       "      <td>5.218025</td>\n",
       "      <td>4.892596</td>\n",
       "      <td>1.180235</td>\n",
       "      <td>7.132350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CH1        CH2        CH3        CH4        CH5       CH6       CH7  \\\n",
       "0   -6.356582  -4.183985 -10.746856  -6.267521  -3.797519 -2.302936 -1.252548   \n",
       "1   -6.945604  -3.306842 -10.491301  -5.355060  -4.204535 -2.451794 -1.078876   \n",
       "2   -7.374339  -3.210059 -10.944341  -5.595750  -5.207158 -2.915861 -1.167011   \n",
       "3   -8.022699  -4.214879 -12.047193  -7.136577  -6.670796 -3.609811 -1.532931   \n",
       "4   -9.317593  -6.341699 -13.596499  -9.721630  -8.280229 -4.351480 -2.061769   \n",
       "5  -11.347833  -9.045066 -15.138646 -12.573982  -9.571043 -4.902422 -2.538497   \n",
       "6  -13.727580 -11.357480 -16.027704 -14.608987 -10.085396 -5.042579 -2.739409   \n",
       "7  -15.657780 -12.296570 -15.635710 -14.858233  -9.544531 -4.646577 -2.540309   \n",
       "8  -16.065189 -11.165067 -13.529844 -12.794677  -7.907675 -3.700201 -1.956622   \n",
       "9  -13.888465  -7.641836  -9.538939  -8.422123  -5.318664 -2.273712 -1.096104   \n",
       "10  -8.588481  -1.863644  -3.815619  -2.247339  -2.053280 -0.499184 -0.085704   \n",
       "11  -0.701868   5.383435   3.051153   4.742328   1.470267  1.421057  0.960833   \n",
       "12   8.026706  12.599540   9.969863  11.198011   4.698724  3.209242  1.933492   \n",
       "13  15.225856  17.975757  15.551407  15.781702   7.075807  4.568995  2.729387   \n",
       "14  18.898332  20.188929  18.620399  17.699510   8.263268  5.296659  3.285915   \n",
       "15  18.268234  19.040525  18.739086  17.047463   8.284639  5.369881  3.608497   \n",
       "16  14.042345  15.510717  16.433205  14.716613   7.481322  4.946508  3.744095   \n",
       "17   8.078262  11.279956  13.003575  11.953432   6.343219  4.290719  3.731271   \n",
       "18   2.714044   8.097053  10.056000   9.890903   5.352487  3.681812  3.585728   \n",
       "19  -0.070165   7.260290   8.960295   9.256028   4.892503  3.345529  3.329523   \n",
       "20   0.567974   9.223716  10.362699  10.209888   5.165280  3.394858  3.024490   \n",
       "21   3.933700  13.328832  13.868129  12.271233   6.092496  3.776579  2.749331   \n",
       "\n",
       "          CH8        CH9       CH10  ...        CH120      CH121      CH122  \\\n",
       "0   -7.746185   0.471738  -6.161901  ...    -0.020039  -2.472429   8.327164   \n",
       "1   -6.614024   0.476950  -6.373581  ...     1.339626  -1.503563   8.399263   \n",
       "2   -5.487274  -1.091632  -7.736202  ...     2.109172  -0.861245   7.858149   \n",
       "3   -5.089931  -3.724341 -10.143687  ...     1.334883  -1.450996   6.402653   \n",
       "4   -6.095693  -6.715143 -13.108866  ...    -1.394790  -3.764086   3.939228   \n",
       "5   -8.844329  -9.460058 -15.817164  ...    -5.473533  -7.390439   0.964761   \n",
       "6  -13.017964 -11.537113 -17.298231  ...    -9.589652 -11.210074  -1.580985   \n",
       "7  -17.410857 -12.564220 -16.694256  ...   -12.427728 -13.959871  -2.802353   \n",
       "8  -20.102499 -12.029400 -13.548552  ...   -13.070301 -14.628959  -2.124832   \n",
       "9  -19.202040  -9.303129  -8.006057  ...   -10.987175 -12.565097   0.758154   \n",
       "10 -13.846472  -3.961860  -0.838915  ...    -6.072401  -7.641586   5.876706   \n",
       "11  -4.814419   3.748668   6.728548  ...     1.013180  -0.602847  12.664744   \n",
       "12   5.631706  12.654003  13.333624  ...     8.645466   6.836127  19.674685   \n",
       "13  14.566747  20.819525  17.839839  ...    14.736543  12.552038  24.906468   \n",
       "14  19.454721  26.151698  19.655002  ...    17.738284  15.013199  26.742834   \n",
       "15  19.031152  27.226232  18.899846  ...    17.346286  13.957641  24.784764   \n",
       "16  13.790523  23.997702  16.359146  ...    14.361574  10.290956  19.982668   \n",
       "17   5.920555  18.034664  13.212865  ...    10.048368   5.486418  14.098517   \n",
       "18  -1.465411  12.038799  10.631726  ...     5.664882   1.050045   9.040040   \n",
       "19  -5.621351   8.754019   9.387173  ...     2.339594  -1.762937   6.373632   \n",
       "20  -5.369264   9.729211   9.627102  ...     0.909194  -2.231149   6.893173   \n",
       "21  -1.542197  14.510664  10.884648  ...     1.537277  -0.495630  10.211183   \n",
       "\n",
       "        CH123      CH124      CH125      CH126      CH127      CH128  Label  \n",
       "0   -7.159484  -7.407739  -3.116786  -5.184181  -3.349719  -3.648696    0.0  \n",
       "1   -6.161061  -6.157439  -1.761651  -3.521291  -1.393808  -2.770334    0.0  \n",
       "2   -5.850475  -5.879990  -0.573228  -2.107724  -0.457397  -2.487843    0.0  \n",
       "3   -6.514009  -6.857675  -0.646044  -1.901586  -1.285065  -3.199575    0.0  \n",
       "4   -8.125721  -8.944704  -2.570611  -3.342424  -3.797954  -4.744012    0.0  \n",
       "5  -10.181576 -11.441631  -5.938907  -6.254167  -6.961302  -6.451284    0.0  \n",
       "6  -11.847825 -13.305220  -9.562409  -9.947935  -9.398963  -7.528073    0.0  \n",
       "7  -12.287063 -13.552210 -12.089572 -13.301622 -10.268582  -7.472611    0.0  \n",
       "8  -10.925930 -11.589879 -12.423819 -14.970184  -9.581035  -6.211742    0.0  \n",
       "9   -7.576129  -7.372830  -9.866407 -13.868813  -7.760791  -3.935865    0.0  \n",
       "10  -2.504855  -1.445197  -4.316758  -9.762311  -5.106575  -0.913842    0.0  \n",
       "11   3.505577   5.106148   3.389623  -3.515525  -1.797644   2.555682    0.0  \n",
       "12   9.263953  10.891760  11.434667   3.302683   1.784651   6.105044    0.0  \n",
       "13  13.552375  14.696814  17.640820   9.112618   4.984274   9.304216    0.0  \n",
       "14  15.638798  15.989765  20.442837  12.751268   7.183486  11.717796    0.0  \n",
       "15  15.568981  15.134221  19.546347  13.620667   8.133699  12.967944    0.0  \n",
       "16  14.055664  13.159627  15.863175  11.837364   7.938288  12.866485    0.0  \n",
       "17  12.096164  11.271009  10.961098   8.404723   6.800525  11.593885    0.0  \n",
       "18  10.583893  10.395355   6.505560   5.002209   4.989876   9.746565    0.0  \n",
       "19  10.065965  10.926466   3.827274   3.160027   2.995303   8.098228    0.0  \n",
       "20  10.617747  12.648485   3.531653   3.370974   1.532905   7.198523    0.0  \n",
       "21  11.816085  14.814487   5.218025   4.892596   1.180235   7.132350    0.0  \n",
       "\n",
       "[22 rows x 129 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('FinalData.csv')\n",
    "df.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing :\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Label'], axis = 1).values\n",
    "Y = df['Label']\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1726 278334]\n",
      " [  1461 290429]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.01      0.01    280060\n",
      "        1.0       0.51      0.99      0.67    291890\n",
      "\n",
      "avg / total       0.53      0.51      0.35    571950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainedmodel = LogisticRegression().fit(X_Train,Y_Train)\n",
    "predictions =trainedmodel.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictions))\n",
    "print(classification_report(Y_Test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainedforest = RandomForestClassifier().fit(X_Train,Y_Train)\n",
    "# predictionforest = trainedforest.predict(X_Test)\n",
    "# print(confusion_matrix(Y_Test,predictionforest))\n",
    "# print(classification_report(Y_Test,predictionforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 42802 237258]\n",
      " [ 35739 256151]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.15      0.24    280060\n",
      "        1.0       0.52      0.88      0.65    291890\n",
      "\n",
      "avg / total       0.53      0.52      0.45    571950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainedsvm = svm.LinearSVC().fit(X_Train, Y_Train)\n",
    "predictionsvm = trainedsvm.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionsvm))\n",
    "print(classification_report(Y_Test,predictionsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[223460  56600]\n",
      " [ 57227 234663]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.80      0.80    280060\n",
      "        1.0       0.81      0.80      0.80    291890\n",
      "\n",
      "avg / total       0.80      0.80      0.80    571950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\n",
    "predictionstree = trainedtree.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionstree))\n",
    "print(classification_report(Y_Test,predictionstree))\n",
    "# predictionstree = trainedtree.predict_proba(X_Test)\n",
    "# print(predictionstree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1726 278334]\n",
      " [  1461 290429]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.01      0.01    280060\n",
      "        1.0       0.51      0.99      0.67    291890\n",
      "\n",
      "avg / total       0.53      0.51      0.35    571950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainedlda = LinearDiscriminantAnalysis().fit(X_Train, Y_Train)\n",
    "predictionlda = trainedlda.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionlda))\n",
    "print(classification_report(Y_Test,predictionlda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[114035 166025]\n",
      " [ 65815 226075]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.63      0.41      0.50    280060\n",
      "        1.0       0.58      0.77      0.66    291890\n",
      "\n",
      "avg / total       0.60      0.59      0.58    571950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainednb = GaussianNB().fit(X_Train, Y_Train)\n",
    "predictionnb = trainednb.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionnb))\n",
    "print(classification_report(Y_Test,predictionnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "[[218079  61981]\n",
      " [227892  63998]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.78      0.60    280060\n",
      "        1.0       0.51      0.22      0.31    291890\n",
      "\n",
      "avg / total       0.50      0.49      0.45    571950\n",
      "\n",
      "Decision Tree\n",
      "[[141329 138731]\n",
      " [139384 152506]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.50      0.50    280060\n",
      "        1.0       0.52      0.52      0.52    291890\n",
      "\n",
      "avg / total       0.51      0.51      0.51    571950\n",
      "\n",
      "Random Forest\n",
      "[[141085 138975]\n",
      " [132700 159190]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.50      0.51    280060\n",
      "        1.0       0.53      0.55      0.54    291890\n",
      "\n",
      "avg / total       0.52      0.53      0.52    571950\n",
      "\n",
      "Logistic Regression\n",
      "[[     9 280051]\n",
      " [    60 291830]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.13      0.00      0.00    280060\n",
      "        1.0       0.51      1.00      0.68    291890\n",
      "\n",
      "avg / total       0.32      0.51      0.34    571950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2,svd_solver='full')\n",
    "X_pca = pca.fit_transform(X)\n",
    "# print(pca.explained_variance_)\n",
    "\n",
    "X_reduced, X_test_reduced, Y_Train, Y_Test = train_test_split(X_pca, Y, test_size = 0.30, random_state = 101)\n",
    "\n",
    "# pca = PCA(n_components=2,svd_solver='full')\n",
    "# X_reduced = pca.fit_transform(X_Train)\n",
    "#X_reduced = TSNE(n_components=2).fit_transform(X_Train, Y_Train)\n",
    "\n",
    "trainednb = GaussianNB().fit(X_reduced, Y_Train)\n",
    "trainedtree = tree.DecisionTreeClassifier().fit(X_reduced, Y_Train)\n",
    "trainedforest = RandomForestClassifier(n_estimators=700).fit(X_reduced,Y_Train)\n",
    "trainedmodel = LogisticRegression().fit(X_reduced,Y_Train)\n",
    "\n",
    "# pca = PCA(n_components=2,svd_solver='full')\n",
    "# X_test_reduced = pca.fit_transform(X_Test)\n",
    "#X_test_reduced = TSNE(n_components=2).fit_transform(X_Test, Y_Test)\n",
    "\n",
    "print('Naive Bayes')\n",
    "predictionnb = trainednb.predict(X_test_reduced)\n",
    "print(confusion_matrix(Y_Test,predictionnb))\n",
    "print(classification_report(Y_Test,predictionnb))\n",
    "\n",
    "print('Decision Tree')\n",
    "predictionstree = trainedtree.predict(X_test_reduced)\n",
    "print(confusion_matrix(Y_Test,predictionstree))\n",
    "print(classification_report(Y_Test,predictionstree))\n",
    "\n",
    "print('Random Forest')\n",
    "predictionforest = trainedforest.predict(X_test_reduced)\n",
    "print(confusion_matrix(Y_Test,predictionforest))\n",
    "print(classification_report(Y_Test,predictionforest))\n",
    "\n",
    "print('Logistic Regression')\n",
    "predictions =trainedmodel.predict(X_test_reduced)\n",
    "print(confusion_matrix(Y_Test,predictions))\n",
    "print(classification_report(Y_Test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 58720256 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-36c3c980d63d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrainednb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_Train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrainedtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_reduced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_Train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrainedforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_Train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtrainedmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_Train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 328\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree._add_node\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree._resize_c\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_utils.pyx\u001b[0m in \u001b[0;36msklearn.tree._utils.safe_realloc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 58720256 bytes"
     ]
    }
   ],
   "source": [
    "# Thanks to: https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n",
    "# Plotting decision regions\n",
    "\n",
    "reduced_data = X_reduced\n",
    "\n",
    "trainednb = GaussianNB().fit(reduced_data, Y_Train)\n",
    "trainedtree = tree.DecisionTreeClassifier().fit(X_reduced, Y_Train)\n",
    "trainedforest = RandomForestClassifier(n_estimators=700).fit(reduced_data,Y_Train)\n",
    "trainedmodel = LogisticRegression().fit(reduced_data,Y_Train)\n",
    "\n",
    "\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1]),\n",
    "                        [trainednb, trainedtree, trainedforest, trainedmodel],\n",
    "                        ['Naive Bayes Classifier', 'Decision Tree',\n",
    "                         'Random Forest', 'Logistic Regression']):\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z,cmap=plt.cm.coolwarm, alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].scatter(reduced_data[:, 0], reduced_data[:, 1], c=Y_Train,\n",
    "                                  s=20, edgecolor='k')\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
